{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES INSTALLATION OF https://github.com/johli/seqprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, GlobalMaxPooling1D, concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "from seqprop.visualization import *\n",
    "from seqprop.generator import *\n",
    "from seqprop.predictor import *\n",
    "from seqprop.optimizer import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "class IdentityEncoder(iso.SequenceEncoder) :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        super(IdentityEncoder, self).__init__('identity', (seq_len, len(channel_map)))\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_predictor(model_path, library_context=None) :\n",
    "    \n",
    "    n_filters = 600\n",
    "    filt_sizes = [25,11,7]\n",
    "    n_dense = 64\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    sequence_input = Input(shape=(145, 4),name=\"pat_input\")  \n",
    "    convs = [None]*len(filt_sizes)\n",
    "    \n",
    "    for i in range(len(filt_sizes)):\n",
    "        conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear', name = \"pat_conv_\" + str(i) + \"_copy\", trainable=False)(sequence_input)\n",
    "        batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i) + \"_copy\", trainable=False)(conv1)\n",
    "        relu1           = Activation('relu',name = \"pat_relu_\" + str(i) + \"_copy\")(batchnorm1)\n",
    "        convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i) + \"_copy\")(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i) + \"_copy\")(relu1))\n",
    "    \n",
    "    concat1           = concatenate(convs,name=\"pat_concat_layer_copy\")\n",
    "\n",
    "    dense           = Dense(n_dense,activation='relu',name=\"pat_dense_copy\", trainable=False)(concat1)\n",
    "    output          = Dense(2,activation='linear',name=\"pat_output_copy\", trainable=False)(dense)\n",
    "\n",
    "    saved_model = Model(inputs=sequence_input,outputs=output)\n",
    "    saved_model.compile(optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), loss=\"mse\")\n",
    "    \n",
    "\n",
    "    saved_model.load_weights(model_path)\n",
    "\n",
    "    def _initialize_predictor_weights(predictor_model, saved_model=saved_model) :\n",
    "        \n",
    "        #Load pre-trained model\n",
    "        predictor_layers = [\"pat_conv_0\", \"pat_conv_1\", \"pat_conv_2\", \"pat_batchnorm_0\", \n",
    "                       \"pat_batchnorm_1\", \"pat_batchnorm_2\", \"pat_dense\", \"pat_output\"]\n",
    "            \n",
    "        for layer in predictor_layers:\n",
    "            predictor_model.get_layer(layer).set_weights(saved_model.get_layer(layer + \"_copy\").get_weights())\n",
    "\n",
    "\n",
    "    def _load_predictor_func(sequence_input) :\n",
    "        #DragoNN parameters\n",
    "        seq_length = 145\n",
    "        n_filters = 600\n",
    "        filt_sizes = [25,11,7]\n",
    "        n_dense = 64\n",
    "        dropout_rate = 0.1\n",
    "        \n",
    "        seq_input_shape = (seq_length, 4, 1)\n",
    "        n_tasks = 1\n",
    "\n",
    "        #Define model layers\n",
    "        permute_input = Lambda(lambda x: x[..., -1])\n",
    "        \n",
    "        permuted_input = permute_input(sequence_input)\n",
    "        for i in range(len(filt_sizes)):\n",
    "            conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear',\n",
    "                                     name = \"pat_conv_\" + str(i), trainable=False)(permuted_input)\n",
    "            batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i), trainable=False)(conv1)\n",
    "            relu1           = Activation('relu',name = \"pat_relu_\" + str(i))(batchnorm1)\n",
    "            convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i))(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i))(relu1))\n",
    "        \n",
    "        concat1           = concatenate(convs,name=\"pat_concat_layer\")\n",
    "    \n",
    "        dense           = Dense(n_dense,activation='relu',name=\"pat_dense\", trainable=False)(concat1)\n",
    "        output          = Dense(2,activation='linear',name=\"pat_output\", trainable=False)(dense)\n",
    "\n",
    "        #Execute functional model definition\n",
    "        predictor_inputs = []\n",
    "        predictor_outputs = [output]\n",
    "\n",
    "        return predictor_inputs, predictor_outputs, _initialize_predictor_weights\n",
    "\n",
    "    return _load_predictor_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictor_model(model_path) :\n",
    "    \n",
    "    n_filters = 600\n",
    "    filt_sizes = [25,11,7]\n",
    "    n_dense = 64\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    sequence_input = Input(shape=(145, 4),name=\"pat_input\")  \n",
    "    convs = [None]*len(filt_sizes)\n",
    "    \n",
    "    for i in range(len(filt_sizes)):\n",
    "        conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear', name = \"pat_conv_\" + str(i) + \"_copy\", trainable=False)(sequence_input)\n",
    "        batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i) + \"_copy\", trainable=False)(conv1)\n",
    "        relu1           = Activation('relu',name = \"pat_relu_\" + str(i) + \"_copy\")(batchnorm1)\n",
    "        convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i) + \"_copy\")(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i) + \"_copy\")(relu1))\n",
    "    \n",
    "    concat1           = concatenate(convs,name=\"pat_concat_layer_copy\")\n",
    "\n",
    "    dense           = Dense(n_dense,activation='relu',name=\"pat_dense_copy\", trainable=False)(concat1)\n",
    "    output          = Dense(2,activation='linear',name=\"pat_output_copy\", trainable=False)(dense)\n",
    "\n",
    "    saved_model = Model(inputs=sequence_input,outputs=output)\n",
    "    saved_model.compile(optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), loss=\"mse\")\n",
    "    \n",
    "\n",
    "    saved_model.load_weights(model_path)\n",
    "    return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "acgt_encoder = IdentityEncoder(145, {'A':0, 'C':1, 'G':2, 'T':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punish_margin_conv_activity(activity_margin=1.) :\n",
    "    \n",
    "    def _penalty(conv_out) :\n",
    "        \n",
    "        total_conv_out = K.abs(K.sum(conv_out, axis=-2))\n",
    "        \n",
    "        margin_cost = K.switch(total_conv_out < K.constant(activity_margin, shape=(1,)), K.zeros_like(total_conv_out), total_conv_out - K.constant(activity_margin, shape=(1,)))\n",
    "        \n",
    "        return K.mean(margin_cost, -1)\n",
    "    \n",
    "    return _penalty\n",
    "\n",
    "#Define target isoform loss function\n",
    "def get_earthmover_loss(target_output_ixs, pwm_start=0, pwm_end=70, pwm_target_bits=1.8, pwm_entropy_weight=0.0, conv_1_penalty=0.0, conv_1_margin=1.0, conv_2_penalty=0.0, conv_2_margin=1.0, conv_3_penalty=0.0, conv_3_margin=1.0) :\n",
    "    \n",
    "    punish_c = 0.0\n",
    "    punish_g = 0.0\n",
    "    \n",
    "    entropy_mse = get_margin_entropy(pwm_start=pwm_start, pwm_end=pwm_end, min_bits=pwm_target_bits)\n",
    "    \n",
    "    punish_c_func = get_punish_c(pwm_start=pwm_start, pwm_end=pwm_end)\n",
    "    punish_g_func = get_punish_g(pwm_start=pwm_start, pwm_end=pwm_end)\n",
    "    \n",
    "\n",
    "    fitness_target = 3.0 ### THIS CAN BE CHANGED, REALISTICALLY SHOULD HAVE BEEN AN ARGUMENT NOT HARD-CODED BUT I WAS A BABY GRAD STUDENT\n",
    "    \n",
    "    def loss_func(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, pred_score = predictor_outputs\n",
    "\n",
    "        #Specify costs\n",
    "\n",
    "        #### THIS SHOULD ALSO HAVE BEEN A PARAMETER BUT THE SUBTRACTION ORDER DETERMINES WHETHER H2K OR K2H IS OPTIMIZED AND IS HARD-CODED HERE\n",
    "        fitness_loss = K.mean(K.maximum(-pred_score[..., 0] + pred_score[..., 1]+fitness_target, K.zeros_like(pred_score[..., 0])), axis=1)\n",
    "        \n",
    "        seq_loss = 0.0\n",
    "        seq_loss += punish_c * K.mean(punish_c_func(sampled_pwm), axis=0)\n",
    "        seq_loss += punish_g * K.mean(punish_g_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        entropy_loss = pwm_entropy_weight * entropy_mse(pwm)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss + seq_loss + entropy_loss\n",
    "\n",
    "        return K.reshape(K.sum(total_loss, axis=0), (1,))\n",
    "    \n",
    "    def val_loss_func(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, pred_score = predictor_outputs\n",
    "\n",
    "        #Specify costs\n",
    "        fitness_loss = K.mean(K.maximum(-pred_score[..., 1] + pred_score[..., 0]+fitness_target, K.zeros_like(pred_score[..., 0])), axis=1)\n",
    "    \n",
    "        seq_loss = 0.0\n",
    "        seq_loss += punish_c * K.mean(punish_c_func(sampled_pwm), axis=0)\n",
    "        seq_loss += punish_g * K.mean(punish_g_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        entropy_loss = pwm_entropy_weight * entropy_mse(pwm)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss + seq_loss + entropy_loss\n",
    "\n",
    "        return K.reshape(K.mean(total_loss, axis=0), (1,))\n",
    "    \n",
    "    return loss_func, val_loss_func\n",
    "\n",
    "\n",
    "def get_nop_transform() :\n",
    "    \n",
    "    def _transform_func(pwm) :\n",
    "        \n",
    "        return pwm\n",
    "    \n",
    "    return _transform_func\n",
    "\n",
    "class ValidationCallback(Callback):\n",
    "    def __init__(self, val_name, val_loss_model, val_steps) :\n",
    "        self.val_name = val_name\n",
    "        self.val_loss_model = val_loss_model\n",
    "        self.val_steps = val_steps\n",
    "        \n",
    "        self.val_loss_history = []\n",
    "        \n",
    "        #Track val loss\n",
    "        self.val_loss_history.append(self.val_loss_model.predict(x=None, steps=self.val_steps)[0])\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}) :\n",
    "        #Track val loss\n",
    "        val_loss_value = self.val_loss_model.predict(x=None, steps=self.val_steps)[0]\n",
    "        self.val_loss_history.append(val_loss_value)\n",
    "\n",
    "#Function for running SeqProp on a set of objectives to optimize\n",
    "def run_seqprop(target_output_ixs_list, sequence_templates, loss_funcs, val_loss_funcs, transform_funcs,model_path, n_sequences=1, n_samples=1, n_valid_samples=1, eval_mode='sample', normalize_logits=False, n_epochs=10, steps_per_epoch=100) :\n",
    "    \n",
    "    n_objectives = len(sequence_templates)\n",
    "    \n",
    "    seqprop_predictors = []\n",
    "    valid_monitors = []\n",
    "    train_histories = []\n",
    "    valid_histories = []\n",
    "    \n",
    "    for obj_ix in range(n_objectives) :\n",
    "        print(\"Optimizing objective \" + str(obj_ix) + '...')\n",
    "        \n",
    "        sequence_template = sequence_templates[obj_ix]\n",
    "        loss_func = loss_funcs[obj_ix]\n",
    "        val_loss_func = val_loss_funcs[obj_ix]\n",
    "        transform_func = transform_funcs[obj_ix]\n",
    "        target_output_ixs = target_output_ixs_list[obj_ix]\n",
    "        \n",
    "        #Build Generator Network\n",
    "        _, seqprop_generator = build_generator(seq_length=len(sequence_template), n_sequences=n_sequences, n_samples=n_samples, sequence_templates=[sequence_template * n_sequences], batch_normalize_pwm=normalize_logits, pwm_transform_func=transform_func, validation_sample_mode='sample')\n",
    "        _, valid_generator = build_generator(seq_length=len(sequence_template), n_sequences=n_sequences, n_samples=n_valid_samples, sequence_templates=[sequence_template * n_sequences], batch_normalize_pwm=normalize_logits, pwm_transform_func=None, validation_sample_mode='sample', master_generator=seqprop_generator)\n",
    "        for layer in valid_generator.layers :\n",
    "            #if 'policy' not in layer.name :\n",
    "            layer.name += \"_valversion\"\n",
    "        \n",
    "        #Build Predictor Network and hook it on the generator PWM output tensor\n",
    "        _, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(model_path, library_context=None), n_sequences=n_sequences, n_samples=n_samples, eval_mode=eval_mode)\n",
    "        _, valid_predictor = build_predictor(valid_generator, load_saved_predictor(model_path, library_context=None), n_sequences=n_sequences, n_samples=n_valid_samples, eval_mode='sample')\n",
    "        for layer in valid_predictor.layers :\n",
    "            if '_valversion' not in layer.name :# and 'policy' not in layer.name :\n",
    "                layer.name += \"_valversion\"\n",
    "        \n",
    "        #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "        _, loss_model = build_loss_model(seqprop_predictor, loss_func)\n",
    "        _, valid_loss_model = build_loss_model(valid_predictor, val_loss_func)\n",
    "        \n",
    "        #Specify Optimizer to use\n",
    "        opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "        #Compile Loss Model (Minimize self)\n",
    "        loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "        def get_logit(p) :\n",
    "            return np.log(p / (1. - p))\n",
    "        \n",
    "#         #Specify callback entities\n",
    "#         #measure_func = lambda pred_outs: np.mean(get_logit(np.expand_dims(pred_outs[0], axis=0) if len(pred_outs[0].shape) <= 2 else pred_outs[0]), axis=0)\n",
    "#         measure_func = lambda pred_outs: np.expand_dims(np.mean(np.expand_dims(pred_outs[0][..., target_output_ixs], axis=0) if len(pred_outs[0].shape) <= 2 else pred_outs[0][..., target_output_ixs], axis=(0, -1)), axis=-1)\n",
    "        \n",
    "#         #train_monitor = FlexibleSeqPropMonitor(predictor=seqprop_predictor, plot_on_train_end=False, plot_every_epoch=False, track_every_step=True, measure_func=measure_func, measure_name='Activity', plot_pwm_start=500, plot_pwm_end=700, sequence_template=sequence_template, plot_pwm_indices=np.arange(n_sequences).tolist(), figsize=(12, 1.0))\n",
    "#         valid_monitor = FlexibleSeqPropMonitor(predictor=valid_predictor, plot_on_train_end=True, plot_every_epoch=False, track_every_step=True, measure_func=measure_func, measure_name='Activity', plot_pwm_start=0, plot_pwm_end=145, sequence_template=sequence_template, plot_pwm_indices=np.arange(n_sequences).tolist(), figsize=(12, 1.0))\n",
    "        \n",
    "#         train_history = ValidationCallback('loss', loss_model, 1)\n",
    "#         valid_history = ValidationCallback('val_loss', valid_loss_model, 1)\n",
    "        \n",
    "#         callbacks =[\n",
    "#             #EarlyStopping(monitor='loss', min_delta=0.001, patience=5, verbose=0, mode='auto'),\n",
    "#             valid_monitor,\n",
    "#             train_history,\n",
    "#             valid_history\n",
    "#         ]\n",
    "        \n",
    "        #Fit Loss Model\n",
    "        _ = loss_model.fit(\n",
    "            [], np.ones((1, 1)), #Dummy training example\n",
    "            epochs=n_epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "#             callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "#         valid_monitor.predictor = None\n",
    "#         train_history.val_loss_model = None\n",
    "#         valid_history.val_loss_model = None\n",
    "        \n",
    "        seqprop_predictors.append(seqprop_predictor)\n",
    "#         valid_monitors.append(valid_monitor)\n",
    "#         train_histories.append(train_history)\n",
    "#         valid_histories.append(valid_history)\n",
    "\n",
    "    return seqprop_predictors#, valid_monitors, train_histories, valid_histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed_value) :\n",
    "    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.set_random_seed(seed_value)\n",
    "\n",
    "    # 5. Configure a new global `tensorflow` session\n",
    "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Run SeqProp Optimization\n",
    "\n",
    "#Specify file path to pre-trained predictor network\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), '')\n",
    "model_dir = f\"{os.getcwd()}\\\\predictor_models\\\\single_predictors\"\n",
    "seq_file_prefix = \"h2k_seqprop_single\"\n",
    "model_basename = 'wide'\n",
    "\n",
    "seq_template = 'N' * 145\n",
    "\n",
    "# rand_seed = 14755\n",
    "\n",
    "#Number of PWMs to generate per objective\n",
    "n_sequences = 10\n",
    "#Number of One-hot sequences to sample from the PWM at each grad step\n",
    "n_samples = 1\n",
    "#Number of epochs per objective to optimize\n",
    "n_epochs = 1\n",
    "#Number of steps (grad updates) per epoch\n",
    "steps_per_epoch = 200\n",
    "#Number of One-hot validation sequences to sample from the PWM\n",
    "n_valid_samples = 10\n",
    "\n",
    "experiment_name_list = ['Sampled-IN']\n",
    "eval_mode_list = ['sample']\n",
    "normalize_logits_list = [True]\n",
    "\n",
    "n_models = 10\n",
    "for model_ix in range(n_models):\n",
    "    model_name = f\"{model_basename}_{model_ix}.h5\"\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    for experiment_name, eval_mode, normalize_logits in zip(experiment_name_list, eval_mode_list, normalize_logits_list) :\n",
    "\n",
    "        print(\"Experiment name = \" + str(experiment_name))\n",
    "        print(\"Eval mode = \" + str(eval_mode))\n",
    "        print(\"Normalize logits = \" + str(normalize_logits))\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "    #     set_seed(rand_seed)\n",
    "\n",
    "        target_output_ixs = [\n",
    "            [0,1]\n",
    "        ]\n",
    "\n",
    "        sequence_templates = [\n",
    "            seq_template\n",
    "        ]\n",
    "\n",
    "        losses, val_losses = zip(*[\n",
    "            get_earthmover_loss(\n",
    "                target_output_ixs[0],\n",
    "                pwm_start=0,\n",
    "                pwm_end=145,\n",
    "                pwm_target_bits=1.8,\n",
    "                pwm_entropy_weight=0.0\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        transforms = [\n",
    "            None\n",
    "        ]\n",
    "\n",
    "        seqprop_predictors = run_seqprop(target_output_ixs, sequence_templates, losses, val_losses,\n",
    "                                         transforms, model_path, n_sequences, n_samples, n_valid_samples,\n",
    "                                         eval_mode, normalize_logits, n_epochs, steps_per_epoch)\n",
    "\n",
    "        seqprop_predictor = seqprop_predictors[0]#, valid_monitors[0], train_histories[0], valid_histories[0]\n",
    "\n",
    "        #Retrieve optimized PWMs and predicted cleavage distributionns\n",
    "        _, optimized_pwm, _, _ = seqprop_predictor.predict(x=None, steps=1)\n",
    "\n",
    "        consensus_seqs = []\n",
    "        for i in range(optimized_pwm.shape[0]) :\n",
    "            consensus_seq = ''\n",
    "            for j in range(optimized_pwm.shape[1]) :\n",
    "                max_nt_ix = np.argmax(optimized_pwm[i, j, :, 0])\n",
    "                if max_nt_ix == 0 :\n",
    "                    consensus_seq += 'A'\n",
    "                elif max_nt_ix == 1 :\n",
    "                    consensus_seq += 'C'\n",
    "                elif max_nt_ix == 2 :\n",
    "                    consensus_seq += 'G'\n",
    "                elif max_nt_ix == 3 :\n",
    "                    consensus_seq += 'T'\n",
    "\n",
    "            consensus_seqs.append(consensus_seq)\n",
    "\n",
    "        result_dict[experiment_name] = {\n",
    "            'consensus_seqs' : consensus_seqs\n",
    "        }\n",
    "        \n",
    "        \n",
    "        with open(seq_file_prefix + \"_sequences.csv\", \"at\") as f:\n",
    "            seqs = np.array(result_dict['Sampled-IN']['consensus_seqs'])\n",
    "            onehots = np.array([acgt_encoder.encode(seq) for seq in seqs])\n",
    "            predictor = load_predictor_model(model_path)\n",
    "            score_preds = predictor.predict(x=onehots)\n",
    "            h2k_score = score_preds[:,0] - score_preds[:,1]\n",
    "            sort_index = np.argsort(h2k_score)[::-1]\n",
    "            seqs = seqs[sort_index]\n",
    "            h2k_score = h2k_score[sort_index]\n",
    "\n",
    "            f.write(f\"{seqs[0]}, {h2k_score[0]}, seqprop, {model_basename}_{model_ix}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
