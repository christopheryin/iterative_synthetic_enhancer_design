{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import isolearn.io as isoio\n",
    "import isolearn.keras as isol\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "from matplotlib import colors\n",
    "\n",
    "from scipy.optimize import basinhopping, OptimizeResult\n",
    "\n",
    "class IdentityEncoder(iso.SequenceEncoder) :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        super(IdentityEncoder, self).__init__('identity', (seq_len, len(channel_map)))\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "\n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "\n",
    "acgt_encoder = IdentityEncoder(145, {'A':0, 'C':1, 'G':2, 'T':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictor_model(model_path) :\n",
    "    \n",
    "    n_filters = 600\n",
    "    filt_sizes = [25,11,7]\n",
    "    n_dense = 64\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    sequence_input = Input(shape=(145, 4),name=\"pat_input\")  \n",
    "    convs = [None]*len(filt_sizes)\n",
    "    \n",
    "    for i in range(len(filt_sizes)):\n",
    "        conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear', name = \"pat_conv_\" + str(i) + \"_copy\", trainable=False)(sequence_input)\n",
    "        batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i) + \"_copy\", trainable=False)(conv1)\n",
    "        relu1           = Activation('relu',name = \"pat_relu_\" + str(i) + \"_copy\")(batchnorm1)\n",
    "        convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i) + \"_copy\")(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i) + \"_copy\")(relu1))\n",
    "    \n",
    "    concat1           = concatenate(convs,name=\"pat_concat_layer_copy\")\n",
    "\n",
    "    dense           = Dense(n_dense,activation='relu',name=\"pat_dense_copy\", trainable=False)(concat1)\n",
    "    output          = Dense(2,activation='linear',name=\"pat_output_copy\", trainable=False)(dense) # 0 - HepG2, 1 - K562\n",
    "\n",
    "    saved_model = Model(inputs=sequence_input,outputs=output)\n",
    "    saved_model.compile(optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), loss=\"mse\")\n",
    "    \n",
    "\n",
    "    saved_model.load_weights(model_path)\n",
    "    return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _store_sequence(run_dir, run_prefix, seq, curr_iter) :\n",
    "    #Save sequence to file\n",
    "    with open(run_dir + run_prefix + \"_iter_\" + str(int(curr_iter)) + \".txt\", \"a+\") as f :\n",
    "        f.write(seq + \"\\n\")\n",
    "\n",
    "def get_step_func(predictor, sequence_template, acgt_encoder) :\n",
    "    \n",
    "    available_positions = [\n",
    "        j for j in range(len(sequence_template)) if sequence_template[j] == 'N'\n",
    "    ]\n",
    "    \n",
    "    available_nt_dict = {\n",
    "        0 : [1, 2, 3],\n",
    "        1 : [0, 2, 3],\n",
    "        2 : [1, 0, 3],\n",
    "        3 : [1, 2, 0]\n",
    "    }\n",
    "        \n",
    "    _predict_func = get_predict_func(predictor, len(sequence_template))\n",
    "    \n",
    "    def _step_func(x, sequence_template=sequence_template, available_positions=available_positions, available_nt_dict=available_nt_dict) :\n",
    "        \n",
    "        onehot = np.expand_dims(np.expand_dims(x.reshape((len(sequence_template), 4)), axis=0), axis=-1)\n",
    "        \n",
    "        #Choose random position and nucleotide identity\n",
    "        rand_pos = np.random.choice(available_positions)\n",
    "        \n",
    "        curr_nt = np.argmax(onehot[0, rand_pos, :, 0])\n",
    "        rand_nt = np.random.choice(available_nt_dict[curr_nt])\n",
    "        \n",
    "        #Swap nucleotides\n",
    "        onehot[0, rand_pos, :, 0] = 0.\n",
    "        onehot[0, rand_pos, rand_nt, 0] = 1.\n",
    "        \n",
    "        new_x = np.ravel(onehot)\n",
    "        \n",
    "        return new_x\n",
    "    \n",
    "    return _step_func\n",
    "\n",
    "### MODIFY THIS FUNCTION TO SELECT H2K VS K2H OPTIMIZATION ###\n",
    "def get_predict_func(predictor, seq_len) :\n",
    "    \n",
    "    def _predict_func(x, predictor=predictor, seq_len=seq_len) :\n",
    "        \n",
    "        onehot = np.expand_dims(x.reshape((seq_len, 4)), axis=0)\n",
    "        \n",
    "        score_pred = predictor.predict(x=[onehot], batch_size=1)\n",
    "        score_pred = score_pred[0,0]-score_pred[0,1] ############################# for hepg2 - k562\n",
    "#         score_pred = score_pred[0,1]-score_pred[0,0] ############################# for k562 - hepg2\n",
    "\n",
    "        return -score_pred\n",
    "    \n",
    "    return _predict_func\n",
    "\n",
    "def run_simulated_annealing(run_prefix, predictor, sequence_template, acgt_encoder, n_iters=1000, n_iters_per_temperate=100, temperature_init=1.0, temperature_func=None, verbose=False) :\n",
    "    \n",
    "    run_dir = \"./samples/\" + run_prefix + \"/\"\n",
    "    run_prefix = \"intermediate\"\n",
    "    \n",
    "    if not os.path.exists(run_dir): os.makedirs(run_dir)\n",
    "    \n",
    "    if temperature_func is None :\n",
    "        temperature_func = lambda t, curr_iter, t_init=temperature_init, total_iters=n_iters: t\n",
    "    \n",
    "    n_epochs = n_iters // n_iters_per_temperate\n",
    "    \n",
    "    predict_func = get_predict_func(predictor, len(sequence_template))\n",
    "    step_func = get_step_func(predictor, sequence_template, acgt_encoder)\n",
    "    \n",
    "    #Random initialization\n",
    "    random_sequence = ''.join([\n",
    "        sequence_template[j] if sequence_template[j] != 'N' else np.random.choice(['A', 'C', 'G', 'T'])\n",
    "        for j in range(len(sequence_template))\n",
    "    ])\n",
    "\n",
    "    x0 = np.ravel(acgt_encoder.encode(random_sequence))\n",
    "    \n",
    "    x = x0\n",
    "    temperature = temperature_init\n",
    "    \n",
    "    seq_opt = \"\"\n",
    "    tracked_scores = [predict_func(x)]\n",
    "    for epoch_ix in range(n_epochs) :\n",
    "        \n",
    "        x_opt, f_opt = run_basinhopping(x, predict_func, step_func, n_iters=n_iters_per_temperate, temperature=temperature)\n",
    "    \n",
    "        onehot_opt = np.expand_dims(np.expand_dims(x_opt.reshape((len(sequence_template), 4)), axis=0), axis=-1)\n",
    "\n",
    "        seq_opt = acgt_encoder.decode(onehot_opt[0, :, :, 0])\n",
    "        score_opt = -f_opt\n",
    "        tracked_scores.append(score_opt)\n",
    "        \n",
    "        if verbose :\n",
    "            print(\"Iter \" + str((epoch_ix + 1) * n_iters_per_temperate) + \", Temp = \" + str(round(temperature, 4)) + \", Score = \" + str(round(score_opt, 4)) + \"...\")\n",
    "\n",
    "        _store_sequence(run_dir, run_prefix, seq_opt, (epoch_ix + 1) * n_iters_per_temperate)\n",
    "        \n",
    "        x = x_opt\n",
    "        temperature = temperature_func(temperature, (epoch_ix + 1) * n_iters_per_temperate)\n",
    "    \n",
    "    return seq_opt, np.array(tracked_scores)\n",
    "        \n",
    "        \n",
    "def run_basinhopping(x, predict_func, step_func, n_iters=1000, temperature=1.0) :\n",
    "    \n",
    "    def _dummy_min_opt(fun, x0, args=(), **options) :\n",
    "        return OptimizeResult(fun=fun(x0), x=x0, nit=0, nfev=0, success=True)\n",
    "    \n",
    "    minimizer_kwargs = {\n",
    "        'method' : _dummy_min_opt,\n",
    "        'options' : { 'maxiter' : 0 }\n",
    "    }\n",
    "    \n",
    "    opt_res = basinhopping(predict_func, x, minimizer_kwargs=minimizer_kwargs, stepsize=None, niter=n_iters, T=temperature, take_step=step_func)\n",
    "    \n",
    "    return opt_res.x, opt_res.fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run the basinhopping algorithm\n",
    "n_models = 1\n",
    "\n",
    "#Specify file path to pre-trained predictor networks\n",
    "base_path = 'predictor_models/single_predictors'\n",
    "model_basename = 'wide'\n",
    "\n",
    "sequence_template = 'N' * 145\n",
    "\n",
    "n_sequences = 10\n",
    "n_iters = 1000\n",
    "n_iters_per_temperate = 100\n",
    "run_prefix = f\"sim_anneal_df_{n_iters}_iters_k2h_max\"\n",
    "\n",
    "verbose = False\n",
    "\n",
    "t_init = 0.1\n",
    "t_func = lambda t, curr_iter, t_init=t_init, total_iters=n_iters, t_min=0.05, exp_scale=1./0.7: t_init * t_min**(min(float(curr_iter / total_iters) * exp_scale, 1.0))\n",
    "\n",
    "it_space = [0] + [(epoch_ix + 1) * n_iters_per_temperate for epoch_ix in range(n_iters // n_iters_per_temperate)]\n",
    "\n",
    "\n",
    "######################################################\n",
    "with open(run_prefix + \"_sequences.csv\", \"at\") as f:\n",
    "    for i in range(n_models):\n",
    "        K.clear_session()\n",
    "        saved_predictor_model_path = f\"{base_path}/{model_basename}_{i}.h5\"\n",
    "        saved_predictor = load_predictor_model(saved_predictor_model_path)\n",
    "        print(f\"Current model: {saved_predictor_model_path}\")\n",
    "\n",
    "        optimized_seqs = []\n",
    "        optimized_trajs = []\n",
    "\n",
    "        for sequence_ix in range(n_sequences) :\n",
    "\n",
    "            seq, scores = run_simulated_annealing(run_prefix, saved_predictor, sequence_template, acgt_encoder, n_iters=n_iters, n_iters_per_temperate=n_iters_per_temperate, temperature_init=t_init, temperature_func=t_func, verbose=verbose)\n",
    "\n",
    "            optimized_seqs.append(seq)\n",
    "            optimized_trajs.append(scores.reshape(1, -1))\n",
    "\n",
    "\n",
    "        optimized_trajs = np.concatenate(optimized_trajs, axis=0)\n",
    "        sort_index = np.argsort(optimized_trajs[:,-1])[-1] # last index of optimized_trajs is the final score\n",
    "        f.write(f\"{optimized_seqs[sort_index]},{optimized_trajs[sort_index,-1]}, sim_anneal, {model_basename}_{i}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
