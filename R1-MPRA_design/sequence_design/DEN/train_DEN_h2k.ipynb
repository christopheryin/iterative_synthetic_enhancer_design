{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES INSTALLATION OF https://github.com/johli/genesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1835,
     "status": "ok",
     "timestamp": 1633544758043,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "ohiMGbuP0maY",
    "outputId": "4e5a794f-1cc7-43b2-98e5-12592618cf90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, GlobalMaxPooling1D, concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# from genesis_generator_protein import *\n",
    "# from genesis_predictor_protein import *\n",
    "from genesis_visualization import *\n",
    "from genesis_generator import *\n",
    "from genesis_predictor import *\n",
    "from genesis_optimizer import *\n",
    "\n",
    "class IdentityEncoder(iso.SequenceEncoder) :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        super(IdentityEncoder, self).__init__('identity', (seq_len, len(channel_map)))\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1842,
     "status": "ok",
     "timestamp": 1633544761847,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "QY1BgsB90mam"
   },
   "outputs": [],
   "source": [
    "#GENESIS Generator Model definitions\n",
    "\n",
    "def make_deconv_resblock(n_channels=64, window_size=3, stride=1, dilation=1, group_ix=0, layer_ix=0) :\n",
    "\n",
    "    #Initialize res block layers\n",
    "    batch_norm_0 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_0')\n",
    "\n",
    "    relu_0 = Lambda(lambda x: K.relu(x))\n",
    "    \n",
    "    deconv_0 = Conv2DTranspose(n_channels, (1, window_size), strides=(1, stride), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_deconv_0')\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_1')\n",
    "\n",
    "    relu_1 = Lambda(lambda x: K.relu(x))\n",
    "\n",
    "    conv_1 = Conv2D(n_channels, (1, window_size), dilation_rate=(1, dilation), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_conv_1')\n",
    "\n",
    "    skip_deconv_0 = Conv2DTranspose(n_channels, (1, 1), strides=(1, stride), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_skip_deconv_0')\n",
    "    \n",
    "    skip_1 = Lambda(lambda x: x[0] + x[1], name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_skip_1')\n",
    "\n",
    "    #Execute res block\n",
    "    def _resblock_func(input_tensor) :\n",
    "        \n",
    "        batch_norm_0_out = batch_norm_0(input_tensor, training=True)\n",
    "        relu_0_out = relu_0(batch_norm_0_out)\n",
    "        deconv_0_out = deconv_0(relu_0_out)\n",
    "\n",
    "        batch_norm_1_out = batch_norm_1(deconv_0_out, training=True)\n",
    "        relu_1_out = relu_1(batch_norm_1_out)\n",
    "        conv_1_out = conv_1(relu_1_out)\n",
    "        \n",
    "        skip_deconv_0_out = skip_deconv_0(input_tensor)\n",
    "\n",
    "        skip_1_out = skip_1([conv_1_out, skip_deconv_0_out])\n",
    "        \n",
    "        return skip_1_out\n",
    "\n",
    "    return _resblock_func\n",
    "\n",
    "def make_conv_resblock(n_channels=64, window_size=8, dilation=1, group_ix=0, layer_ix=0) :\n",
    "\n",
    "    #Initialize res block layers\n",
    "    batch_norm_0 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_0')\n",
    "\n",
    "    relu_0 = Lambda(lambda x: K.relu(x, alpha=0.0))\n",
    "\n",
    "    conv_0 = Conv2D(n_channels, (1, window_size), dilation_rate=(1, dilation), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_conv_0')\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_1')\n",
    "\n",
    "    relu_1 = Lambda(lambda x: K.relu(x, alpha=0.0))\n",
    "\n",
    "    conv_1 = Conv2D(n_channels, (1, window_size), dilation_rate=(1, dilation), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_conv_1')\n",
    "\n",
    "    skip_1 = Lambda(lambda x: x[0] + x[1], name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_skip_1')\n",
    "\n",
    "    #Execute res block\n",
    "    def _resblock_func(input_tensor) :\n",
    "        batch_norm_0_out = batch_norm_0(input_tensor, training=True)\n",
    "        relu_0_out = relu_0(batch_norm_0_out)\n",
    "        conv_0_out = conv_0(relu_0_out)\n",
    "\n",
    "        batch_norm_1_out = batch_norm_1(conv_0_out, training=True)\n",
    "        relu_1_out = relu_1(batch_norm_1_out)\n",
    "        conv_1_out = conv_1(relu_1_out)\n",
    "\n",
    "        skip_1_out = skip_1([conv_1_out, input_tensor])\n",
    "        \n",
    "        return skip_1_out\n",
    "\n",
    "    return _resblock_func\n",
    "\n",
    "def get_load_generator_network() :\n",
    "\n",
    "    def _load_generator_network(batch_size, sequence_class, n_classes=1, n_out_channels=4, seq_length=145,supply_inputs=False) :\n",
    "\n",
    "        #Generator network parameters\n",
    "        latent_size = 100\n",
    "        sequence_class_onehots = np.eye(n_classes)\n",
    "\n",
    "        #Generator inputs\n",
    "        latent_input_1 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_1') #Input(batch_shape=(batch_size, latent_size))#\n",
    "        latent_input_2 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_2') #Input(batch_shape=(batch_size, latent_size))#\n",
    "        latent_input_1_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_1')(latent_input_1)\n",
    "        latent_input_2_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_2')(latent_input_2)\n",
    "        \n",
    "        class_embedding = Lambda(lambda x: K.gather(K.constant(sequence_class_onehots), K.cast(x[:, 0], dtype='int32')))(sequence_class)\n",
    "\n",
    "        seed_input_1 = Concatenate(axis=-1)([latent_input_1_out, class_embedding])\n",
    "        seed_input_2 = Concatenate(axis=-1)([latent_input_2_out, class_embedding])\n",
    "\n",
    "#         seed_input_1 = latent_input_1_out\n",
    "#         seed_input_2 = latent_input_2_out\n",
    "        \n",
    "        \n",
    "        #Generator network parameters\n",
    "        window_size = 3\n",
    "        \n",
    "        n_groups = 5\n",
    "        n_resblocks_per_group = 2\n",
    "\n",
    "        strides = [2, 2, 2, 2, 2]\n",
    "        use_deconvs = [True, True, True, True, True]\n",
    "        dilations = [1, 1, 1, 1, 1]\n",
    "        channels = [384, 256, 128, 64, 32]\n",
    "        initial_length = 5\n",
    "\n",
    "        #Policy network definition\n",
    "        policy_dense_0 = Dense(initial_length * channels[0], activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_dense_0')\n",
    "        policy_dense_0_reshape = Reshape((1, initial_length, channels[0]))\n",
    "\n",
    "        resblock_deconvs = []\n",
    "        skip_convs = []\n",
    "        skip_adds = []\n",
    "        resblock_groups = []\n",
    "        for group_ix in range(n_groups) :\n",
    "            \n",
    "            if use_deconvs[group_ix] :\n",
    "                resblock_deconvs.append(make_deconv_resblock(n_channels=channels[group_ix], window_size=window_size, stride=strides[group_ix], dilation=1, group_ix=group_ix, layer_ix=0))\n",
    "            else :\n",
    "                resblock_deconvs.append(None)\n",
    "            \n",
    "            if n_resblocks_per_group > 0 :\n",
    "                skip_convs.append(Conv2D(channels[group_ix], (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_generator_skip_conv_' + str(group_ix)))\n",
    "\n",
    "                resblocks = []\n",
    "                for layer_ix in range(n_resblocks_per_group) :\n",
    "                    resblocks.append(make_conv_resblock(n_channels=channels[group_ix], window_size=window_size, dilation=dilations[group_ix], group_ix=group_ix, layer_ix=layer_ix+1))\n",
    "\n",
    "                resblock_groups.append(resblocks)\n",
    "\n",
    "                skip_adds.append(Lambda(lambda x: x[0] + x[1], name='skip_add_group_' + str(group_ix)))\n",
    "        \n",
    "        final_conv = Conv2D(n_out_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_final_conv')\n",
    "\n",
    "        policy_permute = Lambda(lambda x: K.permute_dimensions(x, (0, 2, 3, 1))[:, :seq_length, ...])\n",
    "        \n",
    "        def _generator_func(seed_input) :\n",
    "        \n",
    "            policy_dense_0_out = policy_dense_0_reshape(policy_dense_0(seed_input))\n",
    "#             print(\"policy_dense_0_out \" + str(policy_dense_0_out.shape))\n",
    "\n",
    "            #Connect group of res blocks\n",
    "            output_tensor = policy_dense_0_out\n",
    "            \n",
    "            #Res block group execution\n",
    "            for group_ix in range(n_groups) :\n",
    "#                 print()\n",
    "#                 print(\"-- Group \" + str(group_ix) + \" --\")\n",
    "                \n",
    "                if use_deconvs[group_ix] :\n",
    "                    output_tensor = resblock_deconvs[group_ix](output_tensor)\n",
    "#                     print(\"resblock_deconvs_\" + str(group_ix)+ \" \" + str(output_tensor.shape))\n",
    "                \n",
    "                if n_resblocks_per_group > 0 :\n",
    "                    skip_conv_out = skip_convs[group_ix](output_tensor)\n",
    "#                     print(\"skip_conv_out\" + str(group_ix) + \"     \" + str(skip_conv_out.shape))\n",
    "\n",
    "                    for layer_ix in range(n_resblocks_per_group) :\n",
    "                        output_tensor = resblock_groups[group_ix][layer_ix](output_tensor)\n",
    "\n",
    "                    output_tensor = skip_adds[group_ix]([output_tensor, skip_conv_out])\n",
    "#                     print(\"output_tensor_\" + str(group_ix) + \"    \" + str(output_tensor.shape))\n",
    "\n",
    "            #Final conv out\n",
    "            final_conv_out = final_conv(output_tensor)\n",
    "#             print(\"final conv out \" + str(final_conv_out.shape))\n",
    "\n",
    "            return policy_permute(final_conv_out)\n",
    "\n",
    "        policy_out_1 = _generator_func(seed_input_1)\n",
    "        policy_out_2 = _generator_func(seed_input_2)\n",
    "\n",
    "        return [latent_input_1, latent_input_2], [policy_out_1, policy_out_2], []\n",
    "    \n",
    "    return _load_generator_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1633544762717,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "iPIflrzt0mau"
   },
   "outputs": [],
   "source": [
    "def get_pat_model(n_filters,filt_sizes,n_dense,dropout_rate):\n",
    "    sequence_input = Input(shape=(145, 4),name=\"pat_input\")\n",
    "    convs = [None]*len(filt_sizes)\n",
    "    \n",
    "    for i in range(len(filt_sizes)):\n",
    "        conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear', name = \"pat_conv_\" + str(i))(sequence_input)\n",
    "        batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i))(conv1)\n",
    "        relu1           = Activation('relu',name = \"pat_relu_\" + str(i))(batchnorm1)\n",
    "        convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i))(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i))(relu1))\n",
    "    \n",
    "    concat1           = concatenate(convs,name=\"pat_concat_layer\")\n",
    "\n",
    "    dense           = Dense(n_dense,activation='relu',name=\"pat_dense\")(concat1)\n",
    "    output          = Dense(2,activation='linear',name=\"pat_output\")(dense)\n",
    "\n",
    "    model = Model(inputs=sequence_input,outputs=output)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633544764532,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "Jo9Qm9cgBjJt"
   },
   "outputs": [],
   "source": [
    "def load_predictor_model(model_path) :\n",
    "    \n",
    "    n_filters = 600\n",
    "    filt_sizes = [25,11,7]\n",
    "    n_dense = 64\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    sequence_input = Input(shape=(145, 4),name=\"pat_input\")  \n",
    "    convs = [None]*len(filt_sizes)\n",
    "    \n",
    "    for i in range(len(filt_sizes)):\n",
    "        conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear', name = \"pat_conv_\" + str(i) + \"_copy\", trainable=False)(sequence_input)\n",
    "        batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i) + \"_copy\", trainable=False)(conv1)\n",
    "        relu1           = Activation('relu',name = \"pat_relu_\" + str(i) + \"_copy\")(batchnorm1)\n",
    "        convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i) + \"_copy\")(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i) + \"_copy\")(relu1))\n",
    "    \n",
    "    concat1           = concatenate(convs,name=\"pat_concat_layer_copy\")\n",
    "\n",
    "    dense           = Dense(n_dense,activation='relu',name=\"pat_dense_copy\", trainable=False)(concat1)\n",
    "    output          = Dense(2,activation='linear',name=\"pat_output_copy\", trainable=False)(dense)\n",
    "\n",
    "    saved_model = Model(inputs=sequence_input,outputs=output)\n",
    "    saved_model.compile(optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), loss=\"mse\")\n",
    "    \n",
    "\n",
    "    saved_model.load_weights(model_path)\n",
    "    return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1633544769513,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "3qGudpea0may"
   },
   "outputs": [],
   "source": [
    "def load_saved_predictor(library_context=None) :\n",
    "    \n",
    "    # model_path = \"mpra_predictor.h5\"\n",
    "    # model_path = \"boot_0.h5\"\n",
    "    \n",
    "    n_filters = 600\n",
    "    filt_sizes = [25,11,7]\n",
    "    n_dense = 64\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    sequence_input = Input(shape=(145, 4),name=\"pat_input\")  \n",
    "    convs = [None]*len(filt_sizes)\n",
    "    \n",
    "    for i in range(len(filt_sizes)):\n",
    "        conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear', name = \"pat_conv_\" + str(i) + \"_copy\", trainable=False)(sequence_input)\n",
    "        batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i) + \"_copy\", trainable=False)(conv1)\n",
    "        relu1           = Activation('relu',name = \"pat_relu_\" + str(i) + \"_copy\")(batchnorm1)\n",
    "        convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i) + \"_copy\")(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i) + \"_copy\")(relu1))\n",
    "    \n",
    "    concat1           = concatenate(convs,name=\"pat_concat_layer_copy\")\n",
    "\n",
    "    dense           = Dense(n_dense,activation='relu',name=\"pat_dense_copy\", trainable=False)(concat1)\n",
    "    output          = Dense(2,activation='linear',name=\"pat_output_copy\", trainable=False)(dense)\n",
    "\n",
    "    saved_model = Model(inputs=sequence_input,outputs=output)\n",
    "    saved_model.compile(optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), loss=\"mse\")\n",
    "    \n",
    "\n",
    "    saved_model.load_weights(model_path)\n",
    "\n",
    "    def _initialize_predictor_weights(predictor_model, saved_model=saved_model) :\n",
    "        \n",
    "        #Load pre-trained model\n",
    "        predictor_layers = [\"pat_conv_0\", \"pat_conv_1\", \"pat_conv_2\", \"pat_batchnorm_0\", \n",
    "                       \"pat_batchnorm_1\", \"pat_batchnorm_2\", \"pat_dense\", \"pat_output\"]\n",
    "            \n",
    "        for layer in predictor_layers:\n",
    "            predictor_model.get_layer(layer).set_weights(saved_model.get_layer(layer + \"_copy\").get_weights())\n",
    "            # predictor_model.get_layer(layer + \"_adv\").set_weights(saved_model.get_layer(layer + \"_copy\").get_weights())\n",
    "            \n",
    "    def _dummy(predictor_model):\n",
    "        print(\"dummy\")\n",
    "\n",
    "\n",
    "    def predictor_func(sequence_input,class_input, predictor_inputs, shared_inputs) :\n",
    "        #DragoNN parameters\n",
    "        seq_length = 145\n",
    "        n_filters = 600\n",
    "        filt_sizes = [25,11,7]\n",
    "        n_dense = 64\n",
    "        dropout_rate = 0.1\n",
    "        \n",
    "        seq_input_shape = (seq_length, 4, 1)\n",
    "        n_tasks = 1\n",
    "\n",
    "        #Define model layers\n",
    "        permute_input = Lambda(lambda x: x[..., -1])\n",
    "        \n",
    "        permuted_input = permute_input(sequence_input)\n",
    "        for i in range(len(filt_sizes)):\n",
    "            conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear',\n",
    "                                     name = \"pat_conv_\" + str(i), trainable=False)(permuted_input)\n",
    "            batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i), trainable=False)(conv1)\n",
    "            relu1           = Activation('relu',name = \"pat_relu_\" + str(i))(batchnorm1)\n",
    "            convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i))(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i))(relu1))\n",
    "        \n",
    "        concat1           = concatenate(convs,name=\"pat_concat_layer\")\n",
    "    \n",
    "        dense           = Dense(n_dense,activation='relu',name=\"pat_dense\", trainable=False)(concat1)\n",
    "        output          = Dense(2,activation='linear',name=\"pat_output\", trainable=False)(dense)\n",
    "\n",
    "        #Execute functional model definition\n",
    "        predictor_outputs = [output]\n",
    "\n",
    "        return predictor_outputs\n",
    "\n",
    "    def predictor_func_adv(sequence_input,class_input, predictor_inputs, shared_inputs) :\n",
    "        #DragoNN parameters\n",
    "        seq_length = 145\n",
    "        n_filters = 600\n",
    "        filt_sizes = [25,11,7]\n",
    "        n_dense = 64\n",
    "        dropout_rate = 0.1\n",
    "        \n",
    "        seq_input_shape = (seq_length, 4, 1)\n",
    "        n_tasks = 1\n",
    "\n",
    "        #Define model layers\n",
    "        permute_input = Lambda(lambda x: x[..., -1])\n",
    "        \n",
    "        permuted_input = permute_input(sequence_input)\n",
    "        for i in range(len(filt_sizes)):\n",
    "            conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear',\n",
    "                                     name = \"pat_conv_\" + str(i) + \"_adv\", trainable=False)(permuted_input)\n",
    "            batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i) + \"_adv\", trainable=False)(conv1)\n",
    "            relu1           = Activation('relu',name = \"pat_relu_\" + str(i) + \"_adv\")(batchnorm1)\n",
    "            convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i) + \"_adv\")(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i) + \"_adv\")(relu1))\n",
    "        \n",
    "        concat1           = concatenate(convs,name=\"pat_concat_layer_adv\")\n",
    "    \n",
    "        dense           = Dense(n_dense,activation='relu',name=\"pat_dense_adv\", trainable=False)(concat1)\n",
    "        output          = Dense(2,activation='linear',name=\"pat_output_adv\", trainable=False)(dense)\n",
    "\n",
    "        #Execute functional model definition\n",
    "        predictor_outputs = [output]\n",
    "\n",
    "        return predictor_outputs\n",
    "    \n",
    "    predictor_inputs = []\n",
    "    shared_callables = [] # might not work\n",
    "    return predictor_inputs, shared_callables, predictor_func, predictor_func_adv, _initialize_predictor_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1633544771969,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "g_M1J5Jf0maz"
   },
   "outputs": [],
   "source": [
    "#Define target isoform loss function\n",
    "def get_activity_loss(fitness_target, fitness_weight=1.0, seq_length=145, entropy_min_bits=1.0, entropy_weight=1.0, similarity_margin=0.5, similarity_weight=1.0) :\n",
    "\n",
    "#     margin_entropy_ame = get_target_entropy_sme_masked(pwm_start=0, pwm_end=seq_length, target_bits=entropy_min_bits)\n",
    "#     if entropy_loss_mode == 'margin' :\n",
    "    margin_entropy_ame = get_margin_entropy_ame_masked(pwm_start=0, pwm_end=seq_length, min_bits=entropy_min_bits)\n",
    "    \n",
    "    sample_entropy_ame = get_pwm_margin_sample_entropy_masked(pwm_start=0, pwm_end=seq_length, margin=similarity_margin, shift_1_nt=True)\n",
    "\n",
    "    \n",
    "#     margin_entropy_ame = get_margin_entropy_ame(pwm_start=0, pwm_end=seq_length, min_bits=entropy_min_bits, n_channels=4)\n",
    "#     sample_entropy_ame = get_pwm_margin_sample_entropy(pwm_start=0, pwm_end=seq_length, margin=similarity_margin)\n",
    "    \n",
    "    def _loss_func(loss_tensors) :\n",
    "        _, _, _, sequence_class, pwm_logits_1, pwm_logits_2, pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2, mask, sampled_mask, score_pred = loss_tensors\n",
    "        # _, _, _, sequence_class, pwm_logits_1, pwm_logits_2, pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2, mask, sampled_mask, score_pred, _ = loss_tensors\n",
    "\n",
    "        #Specify costs\n",
    "        # fitness_loss = fitness_weight * K.mean(K.maximum(-score_pred[..., 0], K.zeros_like(score_pred[..., 0])), axis=1)\n",
    "        fitness_loss = fitness_weight * K.mean(K.maximum(-score_pred[..., 0] + score_pred[..., 1]+fitness_target, K.zeros_like(score_pred[..., 0])), axis=1)\n",
    "        # fitness_loss = fitness_weight * (- K.mean(score_pred[..., 0], axis=1) + K.mean(score_pred[..., 1], axis=1))\n",
    "        # fitness_loss = fitness_weight * -K.mean(K.print_tensor(score_pred[..., 0], message=\"score_pred=\"), axis=1)#(- K.mean(score_pred[..., 1], axis=1) + K.mean(score_pred[..., 0], axis=1))\n",
    "        \n",
    "        similarity_loss = similarity_weight * K.mean(sample_entropy_ame(sampled_pwm_1, sampled_pwm_2, sampled_mask), axis=1)\n",
    "        \n",
    "        entropy_loss = entropy_weight * margin_entropy_ame(pwm_1, mask)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss + similarity_loss + entropy_loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    return _loss_func\n",
    "\n",
    "def build_loss_model(predictor_model, loss_func, extra_loss_tensors=[]) :\n",
    "#     print(\"Predictor Model Inputs Cnt: \" + str(len(predictor_model.inputs)))\n",
    "#     print(\"Predictor Model Outputs Cnt: \" + str(len(predictor_model.outputs)))\n",
    "\n",
    "    loss_out = Lambda(lambda out: loss_func(out), output_shape = (1,),name=\"loss_out\")(predictor_model.inputs + predictor_model.outputs + extra_loss_tensors)\n",
    "    print(\"loss out shape: \" + str(loss_out.shape))\n",
    "    loss_model = Model(predictor_model.inputs, loss_out)\n",
    "\n",
    "    return 'loss_model', loss_model\n",
    "\n",
    "#Function for running GENESIS\n",
    "def run_genesis(loss_func, load_predictor_func, accum_iters=1, seq_len=100, batch_size=32, n_samples=1, n_epochs=10, steps_per_epoch=100) :\n",
    "    \n",
    "    #Build Generator Network\n",
    "    _, generator = build_generator(batch_size, seq_len, get_load_generator_network(), n_classes=1, n_samples=n_samples, sequence_templates=None, batch_normalize_pwm=False)\n",
    "#     _, generator = build_generator(batch_size, seq_len, get_load_generator_network(), n_classes=1, n_samples=n_samples, sequence_templates=None, batch_normalize_pwm=False, supply_inputs=True)\n",
    "\n",
    "    #Build Predictor Network and hook it on the generator PWM output tensor\n",
    "    _, predictor = build_predictor_w_adversary(generator, load_predictor_func, batch_size, n_samples=n_samples, eval_mode='sample')\n",
    "\n",
    "    #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "    _, loss_model = build_loss_model(predictor, loss_func)\n",
    "    \n",
    "    #Specify Optimizer to use\n",
    "    #opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.5, beta_2=0.9, clipvalue=0.5)\n",
    "#     opt = AdamAccumulate(lr=0.0001, beta_1=0.5, beta_2=0.9, accum_iters=accum_iters)\n",
    "    \n",
    "    #Compile Loss Model (Minimize self)\n",
    "    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "    #Fit Loss Model\n",
    "#     train_history = loss_model.fit(\n",
    "#         [np.zeros((batch_size, 1)),np.ones((batch_size, 100)),np.ones((batch_size, 100))], np.ones((batch_size, 1)),\n",
    "#         epochs=n_epochs,\n",
    "#         steps_per_epoch=steps_per_epoch,\n",
    "#         batch_size = batch_size\n",
    "#     )\n",
    "    #Fit Loss Model\n",
    "    train_history = loss_model.fit(\n",
    "        [], np.ones((1, 1)),\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    train_history = train_history.history\n",
    "\n",
    "    return generator, predictor, train_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75724,
     "status": "ok",
     "timestamp": 1633545607180,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "9BflO8lM0ma1",
    "outputId": "65809fce-73ca-4d79-bc64-c8dad9e9174b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss out shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 145\n",
    "#Number of PWMs to generate per epoch\n",
    "batch_size = 64\n",
    "\n",
    "#Number of samples\n",
    "n_samples = 1\n",
    "#Number of epochs per objective to optimize\n",
    "n_epochs = 100\n",
    "#Number of steps (grad updates) per epoch\n",
    "steps_per_epoch = 100\n",
    "\n",
    "loss_func = get_activity_loss(\n",
    "    2.5,\n",
    "    fitness_weight=0.1,\n",
    "    seq_length=145,\n",
    "    entropy_min_bits=1.8,\n",
    "    entropy_weight=0.5,\n",
    "    similarity_margin=0.5,\n",
    "    similarity_weight=5.0\n",
    ")\n",
    "\n",
    "save_dir = 'saved_models'\n",
    "\n",
    "for i in range(0,10):\n",
    "    for model_path in [f\"boot_{i}.h5\",  f\"wide_{i}.h5\"]:\n",
    "        generator_model, predictor_model, train_history = run_genesis(loss_func, load_saved_predictor, seq_len=seq_length, batch_size=batch_size, n_samples=n_samples, n_epochs=n_epochs)\n",
    "\n",
    "        generator_model.get_layer('lambda_rand_sequence_class').function = lambda inp: inp\n",
    "        generator_model.get_layer('lambda_rand_input_1').function = lambda inp: inp\n",
    "        generator_model.get_layer('lambda_rand_input_2').function = lambda inp: inp\n",
    "\n",
    "        predictor_model.get_layer('lambda_rand_sequence_class').function = lambda inp: inp\n",
    "        predictor_model.get_layer('lambda_rand_input_1').function = lambda inp: inp\n",
    "        predictor_model.get_layer('lambda_rand_input_2').function = lambda inp: inp\n",
    "\n",
    "        # Save model and weights\n",
    "\n",
    "\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        model_prefix = f\"boot{i}_minp_hepg2-k562_fw01_ft2_5\"\n",
    "\n",
    "        model_name = model_prefix + '_generator.h5'\n",
    "        model_path = os.path.join(save_dir, model_name)\n",
    "        generator_model.save(model_path)\n",
    "        print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhCN-lrQASOd"
   },
   "outputs": [],
   "source": [
    "# generator = load_model(\"/content/saved_models/boot4_minp_hepg2-k562_fw01_ft2_5_generator.h5\", custom_objects={'st_sampled_softmax': st_sampled_softmax, 'st_hardmax_softmax': st_hardmax_softmax},compile=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_DEN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
