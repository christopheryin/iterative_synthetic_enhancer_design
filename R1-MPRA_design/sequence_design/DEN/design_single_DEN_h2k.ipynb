{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES INSTALLATION OF https://github.com/johli/genesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1835,
     "status": "ok",
     "timestamp": 1633544758043,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "ohiMGbuP0maY",
    "outputId": "4e5a794f-1cc7-43b2-98e5-12592618cf90"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, GlobalMaxPooling1D, concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# from genesis_generator_protein import *\n",
    "# from genesis_predictor_protein import *\n",
    "from genesis_visualization import *\n",
    "from genesis_generator import *\n",
    "from genesis_predictor import *\n",
    "from genesis_optimizer import *\n",
    "\n",
    "class IdentityEncoder(iso.SequenceEncoder) :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        super(IdentityEncoder, self).__init__('identity', (seq_len, len(channel_map)))\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1842,
     "status": "ok",
     "timestamp": 1633544761847,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "QY1BgsB90mam"
   },
   "outputs": [],
   "source": [
    "#GENESIS Generator Model definitions\n",
    "\n",
    "def make_deconv_resblock(n_channels=64, window_size=3, stride=1, dilation=1, group_ix=0, layer_ix=0) :\n",
    "\n",
    "    #Initialize res block layers\n",
    "    batch_norm_0 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_0')\n",
    "\n",
    "    relu_0 = Lambda(lambda x: K.relu(x))\n",
    "    \n",
    "    deconv_0 = Conv2DTranspose(n_channels, (1, window_size), strides=(1, stride), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_deconv_0')\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_1')\n",
    "\n",
    "    relu_1 = Lambda(lambda x: K.relu(x))\n",
    "\n",
    "    conv_1 = Conv2D(n_channels, (1, window_size), dilation_rate=(1, dilation), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_conv_1')\n",
    "\n",
    "    skip_deconv_0 = Conv2DTranspose(n_channels, (1, 1), strides=(1, stride), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_skip_deconv_0')\n",
    "    \n",
    "    skip_1 = Lambda(lambda x: x[0] + x[1], name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_skip_1')\n",
    "\n",
    "    #Execute res block\n",
    "    def _resblock_func(input_tensor) :\n",
    "        \n",
    "        batch_norm_0_out = batch_norm_0(input_tensor, training=True)\n",
    "        relu_0_out = relu_0(batch_norm_0_out)\n",
    "        deconv_0_out = deconv_0(relu_0_out)\n",
    "\n",
    "        batch_norm_1_out = batch_norm_1(deconv_0_out, training=True)\n",
    "        relu_1_out = relu_1(batch_norm_1_out)\n",
    "        conv_1_out = conv_1(relu_1_out)\n",
    "        \n",
    "        skip_deconv_0_out = skip_deconv_0(input_tensor)\n",
    "\n",
    "        skip_1_out = skip_1([conv_1_out, skip_deconv_0_out])\n",
    "        \n",
    "        return skip_1_out\n",
    "\n",
    "    return _resblock_func\n",
    "\n",
    "def make_conv_resblock(n_channels=64, window_size=8, dilation=1, group_ix=0, layer_ix=0) :\n",
    "\n",
    "    #Initialize res block layers\n",
    "    batch_norm_0 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_0')\n",
    "\n",
    "    relu_0 = Lambda(lambda x: K.relu(x, alpha=0.0))\n",
    "\n",
    "    conv_0 = Conv2D(n_channels, (1, window_size), dilation_rate=(1, dilation), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_conv_0')\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_1')\n",
    "\n",
    "    relu_1 = Lambda(lambda x: K.relu(x, alpha=0.0))\n",
    "\n",
    "    conv_1 = Conv2D(n_channels, (1, window_size), dilation_rate=(1, dilation), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_conv_1')\n",
    "\n",
    "    skip_1 = Lambda(lambda x: x[0] + x[1], name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_skip_1')\n",
    "\n",
    "    #Execute res block\n",
    "    def _resblock_func(input_tensor) :\n",
    "        batch_norm_0_out = batch_norm_0(input_tensor, training=True)\n",
    "        relu_0_out = relu_0(batch_norm_0_out)\n",
    "        conv_0_out = conv_0(relu_0_out)\n",
    "\n",
    "        batch_norm_1_out = batch_norm_1(conv_0_out, training=True)\n",
    "        relu_1_out = relu_1(batch_norm_1_out)\n",
    "        conv_1_out = conv_1(relu_1_out)\n",
    "\n",
    "        skip_1_out = skip_1([conv_1_out, input_tensor])\n",
    "        \n",
    "        return skip_1_out\n",
    "\n",
    "    return _resblock_func\n",
    "\n",
    "def get_load_generator_network() :\n",
    "\n",
    "    def _load_generator_network(batch_size, sequence_class, n_classes=1, n_out_channels=4, seq_length=145,supply_inputs=False) :\n",
    "\n",
    "        #Generator network parameters\n",
    "        latent_size = 100\n",
    "        sequence_class_onehots = np.eye(n_classes)\n",
    "\n",
    "        #Generator inputs\n",
    "        latent_input_1 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_1') #Input(batch_shape=(batch_size, latent_size))#\n",
    "        latent_input_2 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_2') #Input(batch_shape=(batch_size, latent_size))#\n",
    "        latent_input_1_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_1')(latent_input_1)\n",
    "        latent_input_2_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_2')(latent_input_2)\n",
    "        \n",
    "        class_embedding = Lambda(lambda x: K.gather(K.constant(sequence_class_onehots), K.cast(x[:, 0], dtype='int32')))(sequence_class)\n",
    "\n",
    "        seed_input_1 = Concatenate(axis=-1)([latent_input_1_out, class_embedding])\n",
    "        seed_input_2 = Concatenate(axis=-1)([latent_input_2_out, class_embedding])\n",
    "\n",
    "#         seed_input_1 = latent_input_1_out\n",
    "#         seed_input_2 = latent_input_2_out\n",
    "        \n",
    "        \n",
    "        #Generator network parameters\n",
    "        window_size = 3\n",
    "        \n",
    "        n_groups = 5\n",
    "        n_resblocks_per_group = 2\n",
    "\n",
    "        strides = [2, 2, 2, 2, 2]\n",
    "        use_deconvs = [True, True, True, True, True]\n",
    "        dilations = [1, 1, 1, 1, 1]\n",
    "        channels = [384, 256, 128, 64, 32]\n",
    "        initial_length = 5\n",
    "\n",
    "        #Policy network definition\n",
    "        policy_dense_0 = Dense(initial_length * channels[0], activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_dense_0')\n",
    "        policy_dense_0_reshape = Reshape((1, initial_length, channels[0]))\n",
    "\n",
    "        resblock_deconvs = []\n",
    "        skip_convs = []\n",
    "        skip_adds = []\n",
    "        resblock_groups = []\n",
    "        for group_ix in range(n_groups) :\n",
    "            \n",
    "            if use_deconvs[group_ix] :\n",
    "                resblock_deconvs.append(make_deconv_resblock(n_channels=channels[group_ix], window_size=window_size, stride=strides[group_ix], dilation=1, group_ix=group_ix, layer_ix=0))\n",
    "            else :\n",
    "                resblock_deconvs.append(None)\n",
    "            \n",
    "            if n_resblocks_per_group > 0 :\n",
    "                skip_convs.append(Conv2D(channels[group_ix], (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_generator_skip_conv_' + str(group_ix)))\n",
    "\n",
    "                resblocks = []\n",
    "                for layer_ix in range(n_resblocks_per_group) :\n",
    "                    resblocks.append(make_conv_resblock(n_channels=channels[group_ix], window_size=window_size, dilation=dilations[group_ix], group_ix=group_ix, layer_ix=layer_ix+1))\n",
    "\n",
    "                resblock_groups.append(resblocks)\n",
    "\n",
    "                skip_adds.append(Lambda(lambda x: x[0] + x[1], name='skip_add_group_' + str(group_ix)))\n",
    "        \n",
    "        final_conv = Conv2D(n_out_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_final_conv')\n",
    "\n",
    "        policy_permute = Lambda(lambda x: K.permute_dimensions(x, (0, 2, 3, 1))[:, :seq_length, ...])\n",
    "        \n",
    "        def _generator_func(seed_input) :\n",
    "        \n",
    "            policy_dense_0_out = policy_dense_0_reshape(policy_dense_0(seed_input))\n",
    "#             print(\"policy_dense_0_out \" + str(policy_dense_0_out.shape))\n",
    "\n",
    "            #Connect group of res blocks\n",
    "            output_tensor = policy_dense_0_out\n",
    "            \n",
    "            #Res block group execution\n",
    "            for group_ix in range(n_groups) :\n",
    "#                 print()\n",
    "#                 print(\"-- Group \" + str(group_ix) + \" --\")\n",
    "                \n",
    "                if use_deconvs[group_ix] :\n",
    "                    output_tensor = resblock_deconvs[group_ix](output_tensor)\n",
    "#                     print(\"resblock_deconvs_\" + str(group_ix)+ \" \" + str(output_tensor.shape))\n",
    "                \n",
    "                if n_resblocks_per_group > 0 :\n",
    "                    skip_conv_out = skip_convs[group_ix](output_tensor)\n",
    "#                     print(\"skip_conv_out\" + str(group_ix) + \"     \" + str(skip_conv_out.shape))\n",
    "\n",
    "                    for layer_ix in range(n_resblocks_per_group) :\n",
    "                        output_tensor = resblock_groups[group_ix][layer_ix](output_tensor)\n",
    "\n",
    "                    output_tensor = skip_adds[group_ix]([output_tensor, skip_conv_out])\n",
    "#                     print(\"output_tensor_\" + str(group_ix) + \"    \" + str(output_tensor.shape))\n",
    "\n",
    "            #Final conv out\n",
    "            final_conv_out = final_conv(output_tensor)\n",
    "#             print(\"final conv out \" + str(final_conv_out.shape))\n",
    "\n",
    "            return policy_permute(final_conv_out)\n",
    "\n",
    "        policy_out_1 = _generator_func(seed_input_1)\n",
    "        policy_out_2 = _generator_func(seed_input_2)\n",
    "\n",
    "        return [latent_input_1, latent_input_2], [policy_out_1, policy_out_2], []\n",
    "    \n",
    "    return _load_generator_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1633544762717,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "iPIflrzt0mau"
   },
   "outputs": [],
   "source": [
    "def get_pat_model(n_filters,filt_sizes,n_dense,dropout_rate):\n",
    "    sequence_input = Input(shape=(145, 4),name=\"pat_input\")\n",
    "    convs = [None]*len(filt_sizes)\n",
    "    \n",
    "    for i in range(len(filt_sizes)):\n",
    "        conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear', name = \"pat_conv_\" + str(i))(sequence_input)\n",
    "        batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i))(conv1)\n",
    "        relu1           = Activation('relu',name = \"pat_relu_\" + str(i))(batchnorm1)\n",
    "        convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i))(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i))(relu1))\n",
    "    \n",
    "    concat1           = concatenate(convs,name=\"pat_concat_layer\")\n",
    "\n",
    "    dense           = Dense(n_dense,activation='relu',name=\"pat_dense\")(concat1)\n",
    "    output          = Dense(2,activation='linear',name=\"pat_output\")(dense)\n",
    "\n",
    "    model = Model(inputs=sequence_input,outputs=output)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633544764532,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "Jo9Qm9cgBjJt"
   },
   "outputs": [],
   "source": [
    "def load_predictor_model(model_path) :\n",
    "    \n",
    "    n_filters = 600\n",
    "    filt_sizes = [25,11,7]\n",
    "    n_dense = 64\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    sequence_input = Input(shape=(145, 4),name=\"pat_input\")  \n",
    "    convs = [None]*len(filt_sizes)\n",
    "    \n",
    "    for i in range(len(filt_sizes)):\n",
    "        conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear', name = \"pat_conv_\" + str(i) + \"_copy\", trainable=False)(sequence_input)\n",
    "        batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i) + \"_copy\", trainable=False)(conv1)\n",
    "        relu1           = Activation('relu',name = \"pat_relu_\" + str(i) + \"_copy\")(batchnorm1)\n",
    "        convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i) + \"_copy\")(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i) + \"_copy\")(relu1))\n",
    "    \n",
    "    concat1           = concatenate(convs,name=\"pat_concat_layer_copy\")\n",
    "\n",
    "    dense           = Dense(n_dense,activation='relu',name=\"pat_dense_copy\", trainable=False)(concat1)\n",
    "    output          = Dense(2,activation='linear',name=\"pat_output_copy\", trainable=False)(dense)\n",
    "\n",
    "    saved_model = Model(inputs=sequence_input,outputs=output)\n",
    "    saved_model.compile(optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), loss=\"mse\")\n",
    "    \n",
    "\n",
    "    saved_model.load_weights(model_path)\n",
    "    return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1633544769513,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "3qGudpea0may"
   },
   "outputs": [],
   "source": [
    "def load_saved_predictor(library_context=None) :\n",
    "    \n",
    "    # model_path = \"mpra_predictor.h5\"\n",
    "    # model_path = \"boot_0.h5\"\n",
    "    \n",
    "    n_filters = 600\n",
    "    filt_sizes = [25,11,7]\n",
    "    n_dense = 64\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    sequence_input = Input(shape=(145, 4),name=\"pat_input\")  \n",
    "    convs = [None]*len(filt_sizes)\n",
    "    \n",
    "    for i in range(len(filt_sizes)):\n",
    "        conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear', name = \"pat_conv_\" + str(i) + \"_copy\", trainable=False)(sequence_input)\n",
    "        batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i) + \"_copy\", trainable=False)(conv1)\n",
    "        relu1           = Activation('relu',name = \"pat_relu_\" + str(i) + \"_copy\")(batchnorm1)\n",
    "        convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i) + \"_copy\")(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i) + \"_copy\")(relu1))\n",
    "    \n",
    "    concat1           = concatenate(convs,name=\"pat_concat_layer_copy\")\n",
    "\n",
    "    dense           = Dense(n_dense,activation='relu',name=\"pat_dense_copy\", trainable=False)(concat1)\n",
    "    output          = Dense(2,activation='linear',name=\"pat_output_copy\", trainable=False)(dense)\n",
    "\n",
    "    saved_model = Model(inputs=sequence_input,outputs=output)\n",
    "    saved_model.compile(optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), loss=\"mse\")\n",
    "    \n",
    "\n",
    "    saved_model.load_weights(model_path)\n",
    "\n",
    "    def _initialize_predictor_weights(predictor_model, saved_model=saved_model) :\n",
    "        \n",
    "        #Load pre-trained model\n",
    "        predictor_layers = [\"pat_conv_0\", \"pat_conv_1\", \"pat_conv_2\", \"pat_batchnorm_0\", \n",
    "                       \"pat_batchnorm_1\", \"pat_batchnorm_2\", \"pat_dense\", \"pat_output\"]\n",
    "            \n",
    "        for layer in predictor_layers:\n",
    "            predictor_model.get_layer(layer).set_weights(saved_model.get_layer(layer + \"_copy\").get_weights())\n",
    "            # predictor_model.get_layer(layer + \"_adv\").set_weights(saved_model.get_layer(layer + \"_copy\").get_weights())\n",
    "            \n",
    "    def _dummy(predictor_model):\n",
    "        print(\"dummy\")\n",
    "\n",
    "\n",
    "    def predictor_func(sequence_input,class_input, predictor_inputs, shared_inputs) :\n",
    "        #DragoNN parameters\n",
    "        seq_length = 145\n",
    "        n_filters = 600\n",
    "        filt_sizes = [25,11,7]\n",
    "        n_dense = 64\n",
    "        dropout_rate = 0.1\n",
    "        \n",
    "        seq_input_shape = (seq_length, 4, 1)\n",
    "        n_tasks = 1\n",
    "\n",
    "        #Define model layers\n",
    "        permute_input = Lambda(lambda x: x[..., -1])\n",
    "        \n",
    "        permuted_input = permute_input(sequence_input)\n",
    "        for i in range(len(filt_sizes)):\n",
    "            conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear',\n",
    "                                     name = \"pat_conv_\" + str(i), trainable=False)(permuted_input)\n",
    "            batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i), trainable=False)(conv1)\n",
    "            relu1           = Activation('relu',name = \"pat_relu_\" + str(i))(batchnorm1)\n",
    "            convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i))(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i))(relu1))\n",
    "        \n",
    "        concat1           = concatenate(convs,name=\"pat_concat_layer\")\n",
    "    \n",
    "        dense           = Dense(n_dense,activation='relu',name=\"pat_dense\", trainable=False)(concat1)\n",
    "        output          = Dense(2,activation='linear',name=\"pat_output\", trainable=False)(dense)\n",
    "\n",
    "        #Execute functional model definition\n",
    "        predictor_outputs = [output]\n",
    "\n",
    "        return predictor_outputs\n",
    "\n",
    "    def predictor_func_adv(sequence_input,class_input, predictor_inputs, shared_inputs) :\n",
    "        #DragoNN parameters\n",
    "        seq_length = 145\n",
    "        n_filters = 600\n",
    "        filt_sizes = [25,11,7]\n",
    "        n_dense = 64\n",
    "        dropout_rate = 0.1\n",
    "        \n",
    "        seq_input_shape = (seq_length, 4, 1)\n",
    "        n_tasks = 1\n",
    "\n",
    "        #Define model layers\n",
    "        permute_input = Lambda(lambda x: x[..., -1])\n",
    "        \n",
    "        permuted_input = permute_input(sequence_input)\n",
    "        for i in range(len(filt_sizes)):\n",
    "            conv1           = Conv1D(n_filters, filt_sizes[i], padding='same', activation='linear',\n",
    "                                     name = \"pat_conv_\" + str(i) + \"_adv\", trainable=False)(permuted_input)\n",
    "            batchnorm1      = BatchNormalization(axis=-1,name = \"pat_batchnorm_\" + str(i) + \"_adv\", trainable=False)(conv1)\n",
    "            relu1           = Activation('relu',name = \"pat_relu_\" + str(i) + \"_adv\")(batchnorm1)\n",
    "            convs[i]        = Dropout(dropout_rate,name = \"pat_dropout_\" + str(i) + \"_adv\")(GlobalMaxPooling1D(name = \"pat_pool_\" + str(i) + \"_adv\")(relu1))\n",
    "        \n",
    "        concat1           = concatenate(convs,name=\"pat_concat_layer_adv\")\n",
    "    \n",
    "        dense           = Dense(n_dense,activation='relu',name=\"pat_dense_adv\", trainable=False)(concat1)\n",
    "        output          = Dense(2,activation='linear',name=\"pat_output_adv\", trainable=False)(dense)\n",
    "\n",
    "        #Execute functional model definition\n",
    "        predictor_outputs = [output]\n",
    "\n",
    "        return predictor_outputs\n",
    "    \n",
    "    predictor_inputs = []\n",
    "    shared_callables = [] # might not work\n",
    "    return predictor_inputs, shared_callables, predictor_func, predictor_func_adv, _initialize_predictor_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1633544771969,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "g_M1J5Jf0maz"
   },
   "outputs": [],
   "source": [
    "#Define target isoform loss function\n",
    "def get_activity_loss(fitness_target, fitness_weight=1.0, seq_length=145, entropy_min_bits=1.0, entropy_weight=1.0, similarity_margin=0.5, similarity_weight=1.0) :\n",
    "\n",
    "#     margin_entropy_ame = get_target_entropy_sme_masked(pwm_start=0, pwm_end=seq_length, target_bits=entropy_min_bits)\n",
    "#     if entropy_loss_mode == 'margin' :\n",
    "    margin_entropy_ame = get_margin_entropy_ame_masked(pwm_start=0, pwm_end=seq_length, min_bits=entropy_min_bits)\n",
    "    \n",
    "    sample_entropy_ame = get_pwm_margin_sample_entropy_masked(pwm_start=0, pwm_end=seq_length, margin=similarity_margin, shift_1_nt=True)\n",
    "\n",
    "    \n",
    "#     margin_entropy_ame = get_margin_entropy_ame(pwm_start=0, pwm_end=seq_length, min_bits=entropy_min_bits, n_channels=4)\n",
    "#     sample_entropy_ame = get_pwm_margin_sample_entropy(pwm_start=0, pwm_end=seq_length, margin=similarity_margin)\n",
    "    \n",
    "    def _loss_func(loss_tensors) :\n",
    "        _, _, _, sequence_class, pwm_logits_1, pwm_logits_2, pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2, mask, sampled_mask, score_pred = loss_tensors\n",
    "        # _, _, _, sequence_class, pwm_logits_1, pwm_logits_2, pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2, mask, sampled_mask, score_pred, _ = loss_tensors\n",
    "\n",
    "        #Specify costs\n",
    "        fitness_loss = fitness_weight * K.mean(K.maximum(-score_pred[..., 0] + score_pred[..., 1]+fitness_target, K.zeros_like(score_pred[..., 0])), axis=1)\n",
    "\n",
    "        similarity_loss = similarity_weight * K.mean(sample_entropy_ame(sampled_pwm_1, sampled_pwm_2, sampled_mask), axis=1)\n",
    "        \n",
    "        entropy_loss = entropy_weight * margin_entropy_ame(pwm_1, mask)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss + similarity_loss + entropy_loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    return _loss_func\n",
    "\n",
    "def build_loss_model(predictor_model, loss_func, extra_loss_tensors=[]) :\n",
    "#     print(\"Predictor Model Inputs Cnt: \" + str(len(predictor_model.inputs)))\n",
    "#     print(\"Predictor Model Outputs Cnt: \" + str(len(predictor_model.outputs)))\n",
    "\n",
    "    loss_out = Lambda(lambda out: loss_func(out), output_shape = (1,),name=\"loss_out\")(predictor_model.inputs + predictor_model.outputs + extra_loss_tensors)\n",
    "    print(\"loss out shape: \" + str(loss_out.shape))\n",
    "    loss_model = Model(predictor_model.inputs, loss_out)\n",
    "\n",
    "    return 'loss_model', loss_model\n",
    "\n",
    "#Function for running GENESIS\n",
    "def run_genesis(loss_func, load_predictor_func, accum_iters=1, seq_len=100, batch_size=32, n_samples=1, n_epochs=10, steps_per_epoch=100) :\n",
    "    \n",
    "    #Build Generator Network\n",
    "    _, generator = build_generator(batch_size, seq_len, get_load_generator_network(), n_classes=1, n_samples=n_samples, sequence_templates=None, batch_normalize_pwm=False)\n",
    "#     _, generator = build_generator(batch_size, seq_len, get_load_generator_network(), n_classes=1, n_samples=n_samples, sequence_templates=None, batch_normalize_pwm=False, supply_inputs=True)\n",
    "\n",
    "    #Build Predictor Network and hook it on the generator PWM output tensor\n",
    "    _, predictor = build_predictor_w_adversary(generator, load_predictor_func, batch_size, n_samples=n_samples, eval_mode='sample')\n",
    "\n",
    "    #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "    _, loss_model = build_loss_model(predictor, loss_func)\n",
    "    \n",
    "    #Specify Optimizer to use\n",
    "    #opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.5, beta_2=0.9, clipvalue=0.5)\n",
    "#     opt = AdamAccumulate(lr=0.0001, beta_1=0.5, beta_2=0.9, accum_iters=accum_iters)\n",
    "    \n",
    "    #Compile Loss Model (Minimize self)\n",
    "    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "    #Fit Loss Model\n",
    "#     train_history = loss_model.fit(\n",
    "#         [np.zeros((batch_size, 1)),np.ones((batch_size, 100)),np.ones((batch_size, 100))], np.ones((batch_size, 1)),\n",
    "#         epochs=n_epochs,\n",
    "#         steps_per_epoch=steps_per_epoch,\n",
    "#         batch_size = batch_size\n",
    "#     )\n",
    "    #Fit Loss Model\n",
    "    train_history = loss_model.fit(\n",
    "        [], np.ones((1, 1)),\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    train_history = train_history.history\n",
    "\n",
    "    return generator, predictor, train_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75724,
     "status": "ok",
     "timestamp": 1633545607180,
     "user": {
      "displayName": "Christopher Yin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01026764161483300008"
     },
     "user_tz": 420
    },
    "id": "9BflO8lM0ma1",
    "outputId": "65809fce-73ca-4d79-bc64-c8dad9e9174b"
   },
   "outputs": [],
   "source": [
    "seq_length = 145\n",
    "#Number of PWMs to generate per epoch\n",
    "batch_size = 64\n",
    "\n",
    "#Number of samples\n",
    "n_samples = 1\n",
    "#Number of epochs per objective to optimize\n",
    "n_epochs = 100\n",
    "#Number of steps (grad updates) per epoch\n",
    "steps_per_epoch = 100\n",
    "\n",
    "loss_func = get_activity_loss(\n",
    "    2.5,\n",
    "    fitness_weight=0.1,\n",
    "    seq_length=145,\n",
    "    entropy_min_bits=1.8,\n",
    "    entropy_weight=0.5,\n",
    "    similarity_margin=0.5,\n",
    "    similarity_weight=5.0\n",
    ")\n",
    "\n",
    "save_dir = 'saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chhyi\\Desktop\\UW\\Seelig_Lab\\aws\\for_aws\\generator_inference\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "gen_dir = f\"{os.getcwd()}\\..\\generator_models\\single_DENS\"\n",
    "# gen_dir = \"C:\\\\Users\\\\chhyi\\\\Desktop\\\\UW\\\\Seelig_Lab\\\\aws\\\\for_aws\\\\generator_models\\\\single_DENS\".\n",
    "pred_dir = f\"{os.getcwd()}\\..\\predictor_models\"\n",
    "\n",
    "seq_file_prefix = \"h2k_single_DEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading boot model 5\n",
      "Time: 13.440828800201416\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "n = 1000\n",
    "n_slack = 0.05 * n\n",
    "\n",
    "n_ceil = int((n + n_slack) / batch_size) * batch_size + batch_size\n",
    "\n",
    "sequence_class = np.array([0] * n_ceil).reshape(-1, 1) #np.random.uniform(-6, 6, (n, 1)) #\n",
    "acgt_encoder = IdentityEncoder(145, {'A':0, 'C':1, 'G':2, 'T':3})\n",
    "\n",
    "with open(seq_file_prefix + \"_sequences.csv\", \"wt\") as f:\n",
    "\n",
    "\n",
    "    for pred_type in [\"boot\"]:\n",
    "        for gen_i in [5]:#range(10):\n",
    "            t0 = time.time()\n",
    "            K.clear_session()\n",
    "            \n",
    "            print(f\"Loading {pred_type} model {gen_i}\")\n",
    "            # load generator and predictor\n",
    "            gen_model_path = f\"{gen_dir}\\\\{pred_type}_{gen_i}.h5_minp_hepg2-k562_fw01_ft2_5_generator.h5\"\n",
    "            generator = load_model(gen_model_path, custom_objects={'st_sampled_softmax': st_sampled_softmax, 'st_hardmax_softmax': st_hardmax_softmax},compile=False)\n",
    "            pred_model_path = f\"{pred_dir}\\\\{pred_type}_predictors\\\\{pred_type}_{gen_i}.h5\"\n",
    "            saved_predictor =  load_predictor_model(pred_model_path)\n",
    "\n",
    "            # generate 64 sequences\n",
    "            noise_1 = np.random.uniform(-1, 1, (n_ceil, 100))\n",
    "            noise_2 = np.random.uniform(-1, 1, (n_ceil, 100))\n",
    "\n",
    "            pred_outputs = generator.predict([sequence_class, noise_1, noise_2], batch_size=batch_size)\n",
    "\n",
    "            _, _, _, optimized_pwm, _, sampled_pwm, _, _, _ = pred_outputs\n",
    "\n",
    "\n",
    "            onehots = sampled_pwm[:, 0, :, :, 0]\n",
    "\n",
    "            #Make predictions, sort, and take top 10 seqs\n",
    "\n",
    "            score_pred = saved_predictor.predict(x=[onehots], batch_size=batch_size)\n",
    "\n",
    "#             score_pred = np.ravel(score_pred[:, 1] - score_pred[:, 0]) # K2H\n",
    "            score_pred = np.ravel(score_pred[:, 0] - score_pred[:, 1]) # H2K\n",
    "\n",
    "            sort_index = np.argsort(score_pred)[::-1]\n",
    "\n",
    "            onehots = onehots[sort_index][:10]\n",
    "            score_pred = score_pred[sort_index][:10]\n",
    "\n",
    "            seqs = [\n",
    "                acgt_encoder.decode(onehots[i, :, :]) for i in range(onehots.shape[0])\n",
    "            ]\n",
    "            \n",
    "            # write all generated sequences to file\n",
    "            for i in range(10) :\n",
    "                seq = seqs[i]\n",
    "                score = score_pred[i]\n",
    "                f.write(f\"{seq}, {score}, single_DENS_{pred_type}, {pred_type}_{gen_i}\\n\")\n",
    "                \n",
    "            t1 = time.time()\n",
    "            del generator, saved_predictor\n",
    "            print(f\"Time: {t1-t0}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uhCN-lrQASOd"
   },
   "outputs": [],
   "source": [
    "generator = load_model(f\"{gen_dir}\\\\wide_4.h5_minp_k562-hepg2_fw01_ft2_5_generator.h5\", custom_objects={'st_sampled_softmax': st_sampled_softmax, 'st_hardmax_softmax': st_hardmax_softmax},compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_predictor = load_predictor_model(f\"{pred_dir}/wide_predictors/wide_4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n = 1000\n",
    "n_slack = 0.05 * n\n",
    "\n",
    "n_ceil = int((n + n_slack) / batch_size) * batch_size + batch_size\n",
    "\n",
    "sequence_class = np.array([0] * n_ceil).reshape(-1, 1) #np.random.uniform(-6, 6, (n, 1)) #\n",
    "acgt_encoder = IdentityEncoder(145, {'A':0, 'C':1, 'G':2, 'T':3})\n",
    "\n",
    "noise_1 = np.random.uniform(-1, 1, (n_ceil, 100))\n",
    "noise_2 = np.random.uniform(-1, 1, (n_ceil, 100))\n",
    "\n",
    "pred_outputs = generator.predict([sequence_class, noise_1, noise_2], batch_size=batch_size)\n",
    "\n",
    "_, _, _, optimized_pwm, _, sampled_pwm, _, _, _ = pred_outputs\n",
    "\n",
    "pwms = optimized_pwm[:, :, :, 0]\n",
    "onehots = sampled_pwm[:, 0, :, :, 0]\n",
    "\n",
    "#Make predictions using black box model\n",
    "\n",
    "score_pred = saved_predictor.predict(x=[onehots], batch_size=batch_size)\n",
    "\n",
    "# score_pred = np.ravel(score_pred[:, 1] - score_pred[:, 0]) # K2H\n",
    "score_pred = np.ravel(score_pred[:, 0] - score_pred[:, 1]) # H2K\n",
    "\n",
    "sort_index = np.argsort(score_pred)[::-1]\n",
    "\n",
    "pwms = pwms[sort_index][:n]\n",
    "onehots = onehots[sort_index][:n]\n",
    "score_pred = score_pred[sort_index][:n]\n",
    "\n",
    "\n",
    "acgt_encoder = IdentityEncoder(145, {'A':0, 'C':1, 'G':2, 'T':3})\n",
    "\n",
    "\n",
    "seqs = [\n",
    "    acgt_encoder.decode(onehots[i, :, :]) for i in range(onehots.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\"{seq}, {score}, {gen_model_type},{pred_models_used_for_gen}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save sequences to file\n",
    "with open(seq_file_prefix + \"_sequences.csv\", \"wt\") as f:\n",
    "    for i in range(n) :\n",
    "        seq = seqs[i]\n",
    "#         score = score_pred[i]\n",
    "        f.write(\">seq\" + str(i) + \"\\n\" + seq + \"\\n\")\n",
    "#         f.write(f\"{seq}, {score}, single_DENS_boot, boot_0\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x1728 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_n_seqs = 10\n",
    "plot_start = 0\n",
    "plot_end = 145\n",
    "\n",
    "save_figs = False\n",
    "\n",
    "flat_pwms = np.zeros((n, 145))\n",
    "for i in range(n) :\n",
    "    for j in range(145) :\n",
    "        max_nt_ix = np.argmax(onehots[i, j, :])\n",
    "\n",
    "        flat_pwms[i, j] = max_nt_ix + 1\n",
    "\n",
    "flat_pwms = flat_pwms[:plot_n_seqs, plot_start:plot_end]\n",
    "\n",
    "cmap = colors.ListedColormap(['red', 'blue', 'orange', 'darkgreen'])\n",
    "bounds=[1, 2, 3, 4, 5]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "f = plt.figure(figsize=(8, 24))\n",
    "\n",
    "plt.imshow(flat_pwms, aspect='equal', interpolation='nearest', origin='lower', cmap=cmap, norm=norm)\n",
    "\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figs :\n",
    "    plt.savefig(model_prefix + \"_diversity_seqs.png\", transparent=True, dpi=150)\n",
    "    plt.savefig(model_prefix + \"_diversity_seqs.svg\")\n",
    "    plt.savefig(model_prefix + \"_diversity_seqs.eps\")\n",
    "\n",
    "plt.title(\"Bootstrapped Sim Annealed Seqs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_DEN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
